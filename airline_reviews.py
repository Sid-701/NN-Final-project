# -*- coding: utf-8 -*-
"""Airline_reviews.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JLIO4us61x8TdIIYRAlBE5TRNGNFujUX
"""

import pandas as pd
import numpy as np

df= pd.read_csv('/content/Airline_review.csv')

df.info()

print(df.isnull().sum())

# Impute missing values for 'Aircraft' with a placeholder value, e.g., 'Unknown'
df['Aircraft'].fillna('Unknown', inplace=True)

# Impute missing values for categorical columns with the mode (most frequent value)
df['Type Of Traveller'].fillna(df['Type Of Traveller'].mode()[0], inplace=True)

# Impute missing values for 'Route' with a placeholder value, e.g., 'Unknown'
df['Route'].fillna('Unknown', inplace=True)

# Impute missing values for 'Date Flown' with a placeholder value, e.g., 'Unknown'
df['Date Flown'].fillna('Unknown', inplace=True)

# Impute missing values for numeric columns with the mean
numeric_columns = ['Seat Comfort', 'Cabin Staff Service', 'Food & Beverages', 'Ground Service', 'Inflight Entertainment', 'Wifi & Connectivity', 'Value For Money']
for column in numeric_columns:
    df[column].fillna(df[column].mean(), inplace=True)

# Verify that there are no more missing values
print(df.isnull().sum())

def map_n_rating(x):
    res = x
    if x == 'n':
        res = '1'

    return res

df["Overall_Rating"] = df["Overall_Rating"].apply(map_n_rating)

value_for_money_median_map = {
    "1": 1,
    "2": 1,
    "3": 2,
    "4": 2,
    "5": 3,
    "6": 4,
    "7": 4,
    "8": 4,
    "9": 5,
}

df['Median_value_for_money'] = df['Overall_Rating'].map(value_for_money_median_map)

df["Value For Money"] = df.apply(
    lambda row: row['Median_value_for_money'] if np.isnan(row["Value For Money"]) else row["Value For Money"],
    axis=1
)
df=df.drop(columns=['Median_value_for_money'])

df['Seat Type'] = df['Seat Type'].fillna('Economy Class')

df['Review Date'] = pd.to_datetime(df['Review Date'])

df['Type Of Traveller'] = df['Type Of Traveller'].astype('category')
df['Seat Type'] = df['Seat Type'].astype('category')
#df['Recommended'] = df['Recommended'].map({'yes': 1, 'no': 0})

df

import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import files

from wordcloud import WordCloud

reviews_text = ' '.join(df['Review'].astype(str))
wordcloud = WordCloud(width=640, height=480, background_color='white').generate(reviews_text)
plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud: Reviews')
plt.show()

recommended_counts = df['Recommended'].value_counts()
plt.figure(figsize=(8, 8))
plt.pie(recommended_counts, labels=recommended_counts.index, autopct='%1.1f%%', startangle=90, colors=['lightcoral', 'lightgreen'])
plt.gca().add_artist(plt.Circle((0,0),0.70,fc='white'))
plt.title('Donut Plot: Recommended Percentage')
plt.show()

rating_categories = ['Seat Comfort', 'Cabin Staff Service', 'Food & Beverages', 'Ground Service', 'Inflight Entertainment', 'Wifi & Connectivity', 'Value For Money']
ratings_means = df[rating_categories].mean()
angles = [n / float(len(rating_categories)) * 2 * 3.14159 for n in range(len(rating_categories))]
plt.figure(figsize=(6, 8))
plt.polar(angles, ratings_means, marker='o', linestyle='-', color='orange', linewidth=2, alpha=0.7)
plt.fill(angles, ratings_means, color='orange', alpha=0.25)
plt.title('Radar Chart: Average Ratings in Different Categories')
plt.xticks(angles, rating_categories)
plt.show()

plt.figure(figsize=(8, 8))
df['Recommended'].value_counts().plot.pie(autopct='%1.1f%%', colors=['skyblue', 'lightcoral'])
plt.title('Percentage of Recommended Airlines')
plt.show()

top_airlines_overall = df.groupby('Airline Name')['Overall_Rating'].mean().sort_values(ascending=False).head(10)

plt.figure(figsize=(12, 8))
sns.barplot(x=top_airlines_overall.values, y=top_airlines_overall.index, palette='viridis')
plt.title('Top 10 Airlines by Overall Ratings')
plt.xlabel('Overall Rating')
plt.show()

top_airlines_seat_comfort = df.groupby('Airline Name')['Seat Comfort'].mean().sort_values(ascending=False).head(10)

plt.figure(figsize=(8, 6))
sns.barplot(x=top_airlines_seat_comfort.values, y=top_airlines_seat_comfort.index, palette='plasma')
plt.title('Top 10 Airlines by Seat Comfort')
plt.xlabel('Seat Comfort Rating')
plt.show()

top_recommended_airlines = df[df['Recommended'] == 'yes']['Airline Name'].value_counts().head(5)

plt.figure(figsize=(8, 6))
sns.barplot(x=top_recommended_airlines.values, y=top_recommended_airlines.index, palette='pastel')
plt.title('Top 5 Airlines with Highest Recommendations')
plt.xlabel('Number of Recommendations')
plt.show()

df['Type Of Traveller'].info()

plt.figure(figsize=(8, 6))
sns.countplot(x='Type Of Traveller', data=df, palette='viridis')
plt.title('Distribution of Type of Traveller')
plt.xlabel('Type of Traveller')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')
plt.show()

filtered_airlines = df.groupby('Airline Name').filter(lambda x: len(x) >= 50)

# Get the bottom 10 airlines based on average 'Value For Money' rating
bottom_10_airlines = filtered_airlines.groupby('Airline Name')['Value For Money'].mean().nsmallest(10)

# Plot
plt.figure(figsize=(8, 6))
sns.barplot(x=bottom_10_airlines.values, y=bottom_10_airlines.index, palette='viridis')
plt.title('Bottom 10 Airlines with Least Value For Money Rating (>=50 reviews)')
plt.xlabel('Average Value For Money Rating')
plt.ylabel('Airline Name')
plt.show()

from textblob import TextBlob

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

df['Sentiment'] = df['Review'].apply(lambda x: 1 if TextBlob(str(x)).sentiment.polarity > 0 else 0)

df['Review']

positive_reviews = df[df['Sentiment'] > 0]
negative_reviews = df[df['Sentiment'] == 0]

# Count the number of reviews in each sentiment category
positive_count = positive_reviews.shape[0]
negative_count = negative_reviews.shape[0]

# Create a bar plot
plt.bar(['Positive', 'Negative'], [positive_count, negative_count], color=['blue', 'red'])
plt.xlabel('Sentiment')
plt.ylabel('Number of Reviews')
plt.title('Distribution of Sentiments in Reviews')
plt.show()

train_texts, test_texts, y_train, y_test = train_test_split(df['Review'], df['Sentiment'], test_size=0.2, random_state=42)

y_train

tokenizer = Tokenizer(num_words=10000)  # You can adjust the vocabulary size
tokenizer.fit_on_texts(train_texts)

X_train = tokenizer.texts_to_sequences(train_texts)
X_test = tokenizer.texts_to_sequences(test_texts)

X_train = pad_sequences(X_train, maxlen=100)  # You can adjust the sequence length
X_test = pad_sequences(X_test, maxlen=100)

embedding_dim = 50  # You can adjust the embedding dimension
embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))

with open('/content/glove.6B.50d.txt', 'r', encoding='utf-8') as glove_file:
    for line in glove_file:
        values = line.split()
        word = values[0]
        if word in tokenizer.word_index:
            idx = tokenizer.word_index[word]
            embedding_matrix[idx] = np.array(values[1:], dtype='float32')

train_texts_list = train_texts.tolist()

from tensorflow.keras.layers import Embedding, LSTM, MultiHeadAttention, Dense, Input
from tensorflow.keras.models import Sequential

model_transformer = Sequential()
model_transformer.add(Embedding(len(tokenizer.word_index) + 1, embedding_dim, weights=[embedding_matrix], input_length=100, trainable=False))
model_transformer.add(MultiHeadAttention(key_dim=256, num_heads=4))
model_transformer.add(Dense(1, activation='sigmoid'))

model_transformer.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model_transformer.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))

from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten

model_cnn = Sequential()
model_cnn.add(Embedding(len(tokenizer.word_index) + 1, embedding_dim, weights=[embedding_matrix], input_length=100, trainable=False))
model_cnn.add(Conv1D(128, 5, activation='relu'))
model_cnn.add(MaxPooling1D(5))
model_cnn.add(Flatten())
model_cnn.add(Dense(1, activation='sigmoid'))

model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model_cnn.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# Assuming you already have X_train, y_train, X_test, y_test prepared

# Define the CNN model with dropout and early stopping
model_cnn_fine_tuned = Sequential()
model_cnn_fine_tuned.add(Embedding(len(tokenizer.word_index) + 1, embedding_dim, weights=[embedding_matrix], input_length=100, trainable=False))
model_cnn_fine_tuned.add(Conv1D(256, 5, activation='relu'))
model_cnn_fine_tuned.add(MaxPooling1D(2))
model_cnn_fine_tuned.add(Flatten())
model_cnn_fine_tuned.add(Dense(128, activation='relu'))
model_cnn_fine_tuned.add(Dropout(0.5))  # Added dropout for regularization
model_cnn_fine_tuned.add(Dense(1, activation='sigmoid'))

# Compile the model
model_cnn_fine_tuned.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Train the model with early stopping
model_cnn_fine_tuned.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stopping])

loss, accuracy = model_cnn_fine_tuned.evaluate(X_test, y_test)

model_lstm = Sequential()
model_lstm.add(Embedding(len(tokenizer.word_index) + 1, embedding_dim, weights=[embedding_matrix], input_length=100, trainable=False))
model_lstm.add(LSTM(100))
model_lstm.add(Dense(1, activation='sigmoid'))

# Compile the model
model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model_lstm.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))

loss, accuracy = model_cnn.evaluate(X_test, y_test)
print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')

from tensorflow.keras.layers import Bidirectional

model_bidirectional = Sequential()
model_bidirectional.add(Embedding(len(tokenizer.word_index) + 1, embedding_dim, weights=[embedding_matrix], input_length=100, trainable=False))
model_bidirectional.add(Bidirectional(LSTM(100)))
model_bidirectional.add(Dense(1, activation='sigmoid'))

model_bidirectional.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model_bidirectional.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))

loss, accuracy = model_bidirectional.evaluate(X_test, y_test)
print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')

from tensorflow.keras.layers import GRU

model_gru = Sequential()
model_gru.add(Embedding(len(tokenizer.word_index) + 1, embedding_dim, weights=[embedding_matrix], input_length=100, trainable=False))
model_gru.add(GRU(100))
model_gru.add(Dense(1, activation='sigmoid'))

model_gru.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model_gru.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))

loss, accuracy = model_gru.evaluate(X_test, y_test)
print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')

# Define the GRU model
model_gru_finetuned = Sequential()
model_gru_finetuned.add(Embedding(len(tokenizer.word_index) + 1, embedding_dim, weights=[embedding_matrix], input_length=100, trainable=False))
model_gru_finetuned.add(GRU(128, return_sequences=True))  # Increase the number of GRU units
model_gru_finetuned.add(GRU(64))  # Add another GRU layer
model_gru_finetuned.add(Dropout(0.3))  # Add dropout for regularization
model_gru_finetuned.add(Dense(1, activation='sigmoid'))

# Compile the model
model_gru_finetuned.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the finetuned model
model_gru_finetuned.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))

from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix

def calculate_metrics(y_true, y_pred):
    # Calculate precision, recall, and F1 score
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)

    return precision, recall, f1

def print_metrics(model_name, y_true, y_pred):
    precision, recall, f1 = calculate_metrics(y_true, y_pred)

    print(f'Metrics for {model_name}:')
    print(f'Precision: {precision:.4f}')
    print(f'Recall: {recall:.4f}')
    print(f'F1 Score: {f1:.4f}')

y_pred_cnn = model_cnn.predict(X_test)
y_pred_lstm = model_lstm.predict(X_test)
y_pred_bilstm = model_bidirectional.predict(X_test)
y_pred_gru = model_gru.predict(X_test)

y_pred_cnn2 = model_cnn_fine_tuned.predict(X_test)
y_pred_cnn2 = (y_pred_cnn2 > 0.5).astype(int)

y_pred_gru2 = model_gru_finetuned.predict(X_test)
y_pred_gru2=(y_pred_gru2 > 0.5).astype(int)

y_pred_cnn=(y_pred_cnn > 0.5).astype(int)
y_pred_lstm=(y_pred_lstm > 0.5).astype(int)
y_pred_bilstm=(y_pred_bilstm > 0.5).astype(int)
y_pred_gru=(y_pred_gru > 0.5).astype(int)

print_metrics("CNN Fine tuned",y_test,y_pred_cnn2)

print_metrics("GRU Fine tuned",y_test,y_pred_gru2)

print_metrics("LSTM Model:", y_test, y_pred_lstm)
print_metrics("\nBiLSTM Model", y_test, y_pred_bilstm)
print_metrics("\nGRU Model", y_test, y_pred_gru)
print_metrics("\nCNN Model", y_test, y_pred_cnn)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming y_test and y_pred are your true labels and predicted labels, respectively
y_pred = model_cnn.predict(X_test)
y_pred_labels = (y_pred > 0.5).astype(int)

conf_mat = confusion_matrix(y_test, y_pred_cnn2)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Negative', 'Positive'],
            yticklabels=['Negative', 'Positive'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

new_text = 'Moroni to Anjouan. It is a very small airline. My ticket advised me to turn up at 0800hrs which I did. There was confusion at this small airport. I was then directed to the office of AB Aviation which was still closed. It opened at 0900hrs and I was told that the flight had been put back to 1300hrs and that they had tried to contact me. This could not be true as they did not have my phone number. I was with a local guide and he had not been informed either. I presume that I was bumped off. The later flight did operate but as usual, there was confusion at check-in. The flight was only 30mins and there were no further problems. Not a good airline but it is the only one for Comoros.'

new_text_sequence = tokenizer.texts_to_sequences([new_text])
new_text_padded = pad_sequences(new_text_sequence, maxlen=100)

# Make predictions
prediction = model_cnn.predict(new_text_padded)

# If you're dealing with binary classification (sigmoid activation in the output layer)
predicted_label = 1 if prediction > 0.5 else 0

print(f"Predicted Label: {predicted_label}, Confidence: {prediction[0][0]}")

def generate_text(seed_text, model, tokenizer, max_sequence_length, next_words=10):
    for _ in range(next_words):
        token_list = tokenizer.texts_to_sequences([seed_text])[0]
        token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')
        predicted_probs = model.predict(token_list, verbose=0)[0]
        predicted_id = np.argmax(predicted_probs)

        output_word = ""
        for word, index in tokenizer.word_index.items():
            if index == predicted_id:
                output_word = word
                break
        seed_text += " " + output_word
    return seed_text

max_sequence_length = 100  # Update with your actual max_sequence_length
tokenizer = tokenizer  # Replace with your actual tokenizer

# Example usage for each model
seed_text = "I had a great flight"
generated_text_gru = generate_text(seed_text, model_gru, tokenizer, max_sequence_length)
generated_text_cnn = generate_text(seed_text, model_cnn, tokenizer, max_sequence_length)

print(generated_text_gru)